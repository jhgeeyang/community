{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Welcome to the wiki for Machine Learning in Geophysics. This site is maintained by a group of researchers at the Colorado School of Mines interested in applying machine learning to geophysical investigations. How to explore this website On a Desktop: Currently, there are three main sections to this website shown in the navigation tab at the top of the page. Use these tabs to explore the different aspects of the project! Use the sidebar to the right to explore the contents of the current page and use the sidebar to the left to find all the different pages for this active section/tab. Here is a list of the available sections: Welcome!: An introduction to the MLGeophysics group with details on how to contribute. Minutes: Pages for the minutes from our weekly meetings. Resources: A conglomerate of resources we\u2019d like to share.","title":"Welcome!"},{"location":"#welcome","text":"Welcome to the wiki for Machine Learning in Geophysics. This site is maintained by a group of researchers at the Colorado School of Mines interested in applying machine learning to geophysical investigations. How to explore this website On a Desktop: Currently, there are three main sections to this website shown in the navigation tab at the top of the page. Use these tabs to explore the different aspects of the project! Use the sidebar to the right to explore the contents of the current page and use the sidebar to the left to find all the different pages for this active section/tab. Here is a list of the available sections: Welcome!: An introduction to the MLGeophysics group with details on how to contribute. Minutes: Pages for the minutes from our weekly meetings. Resources: A conglomerate of resources we\u2019d like to share.","title":"Welcome!"},{"location":"contributing/","text":"How to Contribute \u00b6 Here is a guide on how to contribute your content. Working in Markdown Here are some tips for writing documentation in Markdown and here is the guid for mkdocs-material markdown specific to this website format. On GitHub \u00b6 We have set up this website to be automatically generated and deployed via Travis-CI so anyone with write privileges can contribute content and have the website immediately reflect their contributions! To get started, go to the MLGeophysics/Community repository and navigate to the docs directory. From there find the appropriate folder for your contribution and create a new file. Want to create a new section (a new tab on the menu bar)? Then simply make a new directory in the docs directory. Add your content in markdown and preview the page before you finish. Finally commit your changes and wait a few minutes for Travis-CI to deploy the website. On Your Own Machine \u00b6 First, clone the MLGeophysics/Community repository: $ git clone https://github.com/MLGeophysics/Community.git $ cd Community Create a Python virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt Now lets serve the website on your machine so that you can add files to the project and immediately see how this looks: ( mlgp ) $ mkdocs serve Open the locally hosted webpage. It should output from the mkdocs serve command and likely would be: http://<YOUR.IP.ADDRESS>:8000 . Make changes and see the results in your web browser. Once you are happy with your contributions, commit and push your changes to the repository!","title":"How to Contribute"},{"location":"contributing/#how-to-contribute","text":"Here is a guide on how to contribute your content. Working in Markdown Here are some tips for writing documentation in Markdown and here is the guid for mkdocs-material markdown specific to this website format.","title":"How to Contribute"},{"location":"contributing/#on-github","text":"We have set up this website to be automatically generated and deployed via Travis-CI so anyone with write privileges can contribute content and have the website immediately reflect their contributions! To get started, go to the MLGeophysics/Community repository and navigate to the docs directory. From there find the appropriate folder for your contribution and create a new file. Want to create a new section (a new tab on the menu bar)? Then simply make a new directory in the docs directory. Add your content in markdown and preview the page before you finish. Finally commit your changes and wait a few minutes for Travis-CI to deploy the website.","title":"On GitHub"},{"location":"contributing/#on-your-own-machine","text":"First, clone the MLGeophysics/Community repository: $ git clone https://github.com/MLGeophysics/Community.git $ cd Community Create a Python virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt Now lets serve the website on your machine so that you can add files to the project and immediately see how this looks: ( mlgp ) $ mkdocs serve Open the locally hosted webpage. It should output from the mkdocs serve command and likely would be: http://<YOUR.IP.ADDRESS>:8000 . Make changes and see the results in your web browser. Once you are happy with your contributions, commit and push your changes to the repository!","title":"On Your Own Machine"},{"location":"minutes/06-nov-18/","text":"06 November 2018 \u00b6 Location: GRLA 107 \u00b6 Attendees clarified a hodge-podge of confusing topics and jargon outlined below. These topics and jargon are regularly encounted in ML literature - they are rarely explained in most papers and can inhibit getting started in ML. Next week, Bane\u2019s project is the primary topic of focus. A VAE summary and PyTorch vs. Keras discussion are backup topics. For the remainder of Fall 2018 we are meeting in Coors Tech 282. Attending: Andy Antoine Thomas Iga Jihyun Jonah Xiaodan Hayden TODO: \u00b6 Post NN video (Thomas) Compile slide for next week (Bane) Begin compiling VAE summary (Iga, Andy, Thomas) Begin compiling PyTorch vs. Keras materials (Jihyun) Problems \u00b6 Input data spans several orders of magnitude, large amplitudes dominate. Consider weighting the residuals (like a weighted norm!). GANs are hard to train? Whomp whomp. Jargon: Variational method specific \u00b6 variational distribution : A probability distribution derived using variational methods. Such as Monte Carlo Markov Chain (MCMC). amortized inference : Removing memoryless/independence property of traditional statistics. mean field inference : ??? ELBO : A method related to variational methods. Evidence Lower BOund. Jargon: General \u00b6 Convolutional vs. Deconvolutional layer meaning : Convolutional layers apply a filter via convolution. Deconvolutional layers are really just the transpose of a convolutional layer. The term deconvolution isn\u2019t used correctly in many ML papers. Back propagation: Application of chain rule to derive gradient for weight updates in a network. What is the difference between Stochastic Gradient Descent and Steepest Gradient Descent? Same thing(ish). Stochastic Gradient Descent is a like a submethod of Steepest Gradient Descent. Stochastic involves using a subset of data, each subset called a batch, to update gradient. Every batch has its own gradient. Updating the gradient this way can help you deal with large amounts of data in a manageable way - and may help get away from local minima. Learning rate: Step-length along direction of gradient you travel while minimizing the loss-function. Epochs: An epoch is complete after all data has been considered in set of gradient updates via Stochastic Gradient Descent. Epochs conclude after all batches have had an opportunity to influence the weights in a network. Batch normalization layers: Normalize the subset of data that is in a specific batch. This actually adds weights that need to be learned during training. It relies on a batch that is large enough to be normally distributed. Could regulate behavior of training. Dropout layers: Upon output, a dropout layer ignore a random subset of weights/activated neurons. Weird form of regularization. Difficult to tune. Antoine discourages using this. Topics of discussion for next week \u00b6 Watch neural network video, post and compile questions. Bane - semester project Touch on VAEs Upcoming/backup topics of discussion \u00b6 Variational Auto-encoders summary and application (Iga, Andy, and Thomas) Arnab - existing project PyTorch vs. Keras","title":"06 November 2018"},{"location":"minutes/06-nov-18/#06-november-2018","text":"","title":"06 November 2018"},{"location":"minutes/06-nov-18/#location-grla-107","text":"Attendees clarified a hodge-podge of confusing topics and jargon outlined below. These topics and jargon are regularly encounted in ML literature - they are rarely explained in most papers and can inhibit getting started in ML. Next week, Bane\u2019s project is the primary topic of focus. A VAE summary and PyTorch vs. Keras discussion are backup topics. For the remainder of Fall 2018 we are meeting in Coors Tech 282. Attending: Andy Antoine Thomas Iga Jihyun Jonah Xiaodan Hayden","title":"Location: GRLA 107"},{"location":"minutes/06-nov-18/#todo","text":"Post NN video (Thomas) Compile slide for next week (Bane) Begin compiling VAE summary (Iga, Andy, Thomas) Begin compiling PyTorch vs. Keras materials (Jihyun)","title":"TODO:"},{"location":"minutes/06-nov-18/#problems","text":"Input data spans several orders of magnitude, large amplitudes dominate. Consider weighting the residuals (like a weighted norm!). GANs are hard to train? Whomp whomp.","title":"Problems"},{"location":"minutes/06-nov-18/#jargon-variational-method-specific","text":"variational distribution : A probability distribution derived using variational methods. Such as Monte Carlo Markov Chain (MCMC). amortized inference : Removing memoryless/independence property of traditional statistics. mean field inference : ??? ELBO : A method related to variational methods. Evidence Lower BOund.","title":"Jargon: Variational method specific"},{"location":"minutes/06-nov-18/#jargon-general","text":"Convolutional vs. Deconvolutional layer meaning : Convolutional layers apply a filter via convolution. Deconvolutional layers are really just the transpose of a convolutional layer. The term deconvolution isn\u2019t used correctly in many ML papers. Back propagation: Application of chain rule to derive gradient for weight updates in a network. What is the difference between Stochastic Gradient Descent and Steepest Gradient Descent? Same thing(ish). Stochastic Gradient Descent is a like a submethod of Steepest Gradient Descent. Stochastic involves using a subset of data, each subset called a batch, to update gradient. Every batch has its own gradient. Updating the gradient this way can help you deal with large amounts of data in a manageable way - and may help get away from local minima. Learning rate: Step-length along direction of gradient you travel while minimizing the loss-function. Epochs: An epoch is complete after all data has been considered in set of gradient updates via Stochastic Gradient Descent. Epochs conclude after all batches have had an opportunity to influence the weights in a network. Batch normalization layers: Normalize the subset of data that is in a specific batch. This actually adds weights that need to be learned during training. It relies on a batch that is large enough to be normally distributed. Could regulate behavior of training. Dropout layers: Upon output, a dropout layer ignore a random subset of weights/activated neurons. Weird form of regularization. Difficult to tune. Antoine discourages using this.","title":"Jargon: General"},{"location":"minutes/06-nov-18/#topics-of-discussion-for-next-week","text":"Watch neural network video, post and compile questions. Bane - semester project Touch on VAEs","title":"Topics of discussion for next week"},{"location":"minutes/06-nov-18/#upcomingbackup-topics-of-discussion","text":"Variational Auto-encoders summary and application (Iga, Andy, and Thomas) Arnab - existing project PyTorch vs. Keras","title":"Upcoming/backup topics of discussion"},{"location":"minutes/22-oct-18/","text":"22 October 2018 \u00b6 Location: GRL 107 \u00b6 Group members discussed impressions from SEG annual presentations relating to machine learning. A crude outline of topics discussed follows. Attending: Andy Antoine Thomas Iga Jihyun Bane Arnab Hayden TODO: find room for weekly meetings @ noon (Thomas) prepare next week\u2019s topic slides (Bane + Hayden + Arnab) Windowed CNN for fault classification \u00b6 Problem : \u201cWhere are the faults?\u201d Xin Ming Wu (OG CWP but UT now) Simple synthetics for generating training data for reflectivity model. Artificial faults induced in data via Mines JTK. Results look convincing. Fault classification given by quantizing: fault existence azimuth and dip Windowed approach, assuming fault goes through the center of voxel (??) Faults reconstructed after classification Classifying signals in ambient seismic \u00b6 Problem : \u201cI\u2019m only interested in certain ambient signals.\u201d Fantine Huot (Stanford SEP) Application to DAS and finding specific types of signals Unsupervised, pick, supervised, then rinse+repeat Extrapolating frequencies for improving FWI \u00b6 Problem : \u201cI need lower frequencies for my FWI to work.\u201d Is this approach physically valid? Hopfield network + boltzmann machine used Q: Why and how do NN deal with RTM artifacts \u201cbetter\u201d? Overall fishy, probably waste of time - we already can do RTM FWI updates via NN \u00b6 Problem \u201cMy FWI model updates take a while and aren\u2019t great.\u201d NN weights updated instead of model updates. Q: Is this like what Andy is doing? ML in interpretation \u00b6 Problem \u201cProper salt interpretation takes too much time and expert knowledge.\u201d Salt interpretation is popular. Top, bottom, and existence interpretations. Problem quantifying error in results. Interpreter still required for QC. 2D overused, 3D is more appropriate. Surface wave attenuation \u00b6 Problem \u201cSomeone get these surface waves out of my data please.\u201d So many GANs. Input: noisy data Output: denoised data given by (insert denoising alg. here) Training data: inputs and outputs from (insert denoising alg. here) You could go unsupervised here. Next steps \u00b6 finding sparse represenations of data (dictionary learning) for denoising and interpolation. (Iga, Arnab, Thomas) denoising and monitoring DAS data (Jihyun) Next week \u00b6 Topics Bane - semester project Hayden - \u201cI have research, I got this.\u201d Arnab - existing project Backup - Iga Resources \u00b6 Getting started with VAEs Open source labelled seismic training data","title":"22 October 2018"},{"location":"minutes/22-oct-18/#22-october-2018","text":"","title":"22 October 2018"},{"location":"minutes/22-oct-18/#location-grl-107","text":"Group members discussed impressions from SEG annual presentations relating to machine learning. A crude outline of topics discussed follows. Attending: Andy Antoine Thomas Iga Jihyun Bane Arnab Hayden TODO: find room for weekly meetings @ noon (Thomas) prepare next week\u2019s topic slides (Bane + Hayden + Arnab)","title":"Location: GRL 107"},{"location":"minutes/22-oct-18/#windowed-cnn-for-fault-classification","text":"Problem : \u201cWhere are the faults?\u201d Xin Ming Wu (OG CWP but UT now) Simple synthetics for generating training data for reflectivity model. Artificial faults induced in data via Mines JTK. Results look convincing. Fault classification given by quantizing: fault existence azimuth and dip Windowed approach, assuming fault goes through the center of voxel (??) Faults reconstructed after classification","title":"Windowed CNN for fault classification"},{"location":"minutes/22-oct-18/#classifying-signals-in-ambient-seismic","text":"Problem : \u201cI\u2019m only interested in certain ambient signals.\u201d Fantine Huot (Stanford SEP) Application to DAS and finding specific types of signals Unsupervised, pick, supervised, then rinse+repeat","title":"Classifying signals in ambient seismic"},{"location":"minutes/22-oct-18/#extrapolating-frequencies-for-improving-fwi","text":"Problem : \u201cI need lower frequencies for my FWI to work.\u201d Is this approach physically valid? Hopfield network + boltzmann machine used Q: Why and how do NN deal with RTM artifacts \u201cbetter\u201d? Overall fishy, probably waste of time - we already can do RTM","title":"Extrapolating frequencies for improving FWI"},{"location":"minutes/22-oct-18/#fwi-updates-via-nn","text":"Problem \u201cMy FWI model updates take a while and aren\u2019t great.\u201d NN weights updated instead of model updates. Q: Is this like what Andy is doing?","title":"FWI updates via NN"},{"location":"minutes/22-oct-18/#ml-in-interpretation","text":"Problem \u201cProper salt interpretation takes too much time and expert knowledge.\u201d Salt interpretation is popular. Top, bottom, and existence interpretations. Problem quantifying error in results. Interpreter still required for QC. 2D overused, 3D is more appropriate.","title":"ML in interpretation"},{"location":"minutes/22-oct-18/#surface-wave-attenuation","text":"Problem \u201cSomeone get these surface waves out of my data please.\u201d So many GANs. Input: noisy data Output: denoised data given by (insert denoising alg. here) Training data: inputs and outputs from (insert denoising alg. here) You could go unsupervised here.","title":"Surface wave attenuation"},{"location":"minutes/22-oct-18/#next-steps","text":"finding sparse represenations of data (dictionary learning) for denoising and interpolation. (Iga, Arnab, Thomas) denoising and monitoring DAS data (Jihyun)","title":"Next steps"},{"location":"minutes/22-oct-18/#next-week","text":"Topics Bane - semester project Hayden - \u201cI have research, I got this.\u201d Arnab - existing project Backup - Iga","title":"Next week"},{"location":"minutes/22-oct-18/#resources","text":"Getting started with VAEs Open source labelled seismic training data","title":"Resources"},{"location":"minutes/30-oct-18/","text":"30 October 2018 \u00b6 Location: GRLA 107 \u00b6 Group members discussed Hayden\u2019s project, which involves making decisions when provided a large number of earth models from a stochastic inversion. An outline of conversation content is below. Based on the meeting, we need to provide a tiny bit more structure to the one-slide conversation starters to bring focus towards the ML aspects of projects. Attending: Andy Thomas Iga Jihyun Arnab Hayden Jonah TODO: \u00b6 Decide on a different meeting time using a Doodle poll (Jihyun) Add Naive Bayes to Resources tab (Hayden) Email website version of slide as a pdf to Thomas (Hayden) Provide slide content guidelines to focus on ML aspects in meetups (Thomas) Making decisions using a suite of stochastically inverted earth models (Hayden) \u00b6 Goal : \u201cWhen provided many earth models that all fit data, how do we make a decision?\u201d Input : Rock properties: density, p-velocity, s-velocity, and porosity Framework : Naive Bayes Binary Classifier Output : High vs. low producer classification Details : - Stochastic inversion input: rock physics model + amplitude-vs-offset - Initial labels derived from user-defined production-based cutoff - Many statistical assumptions in stochastic inversion framework Problems : - Consistent misclassifcation near faults. - Dimensionality reduction problems. Questions : - How does Naive Bayes work? - What are the statistical assumptions in the inversion? Are they violated by Naive Bayes? Deltas : - Consider exploring which of the 5,000 models predicts best? - Consider using other classification algorithms RF, SVM, \u2026 - Consider using smaller number of rock properties as inputs to lower dimensionality Next week \u00b6 Topics - Bane - semester project - Arnab - existing project - Backup - Iga","title":"30 October 2018"},{"location":"minutes/30-oct-18/#30-october-2018","text":"","title":"30 October 2018"},{"location":"minutes/30-oct-18/#location-grla-107","text":"Group members discussed Hayden\u2019s project, which involves making decisions when provided a large number of earth models from a stochastic inversion. An outline of conversation content is below. Based on the meeting, we need to provide a tiny bit more structure to the one-slide conversation starters to bring focus towards the ML aspects of projects. Attending: Andy Thomas Iga Jihyun Arnab Hayden Jonah","title":"Location: GRLA 107"},{"location":"minutes/30-oct-18/#todo","text":"Decide on a different meeting time using a Doodle poll (Jihyun) Add Naive Bayes to Resources tab (Hayden) Email website version of slide as a pdf to Thomas (Hayden) Provide slide content guidelines to focus on ML aspects in meetups (Thomas)","title":"TODO:"},{"location":"minutes/30-oct-18/#making-decisions-using-a-suite-of-stochastically-inverted-earth-models-hayden","text":"Goal : \u201cWhen provided many earth models that all fit data, how do we make a decision?\u201d Input : Rock properties: density, p-velocity, s-velocity, and porosity Framework : Naive Bayes Binary Classifier Output : High vs. low producer classification Details : - Stochastic inversion input: rock physics model + amplitude-vs-offset - Initial labels derived from user-defined production-based cutoff - Many statistical assumptions in stochastic inversion framework Problems : - Consistent misclassifcation near faults. - Dimensionality reduction problems. Questions : - How does Naive Bayes work? - What are the statistical assumptions in the inversion? Are they violated by Naive Bayes? Deltas : - Consider exploring which of the 5,000 models predicts best? - Consider using other classification algorithms RF, SVM, \u2026 - Consider using smaller number of rock properties as inputs to lower dimensionality","title":"Making decisions using a suite of stochastically inverted earth models (Hayden)"},{"location":"minutes/30-oct-18/#next-week","text":"Topics - Bane - semester project - Arnab - existing project - Backup - Iga","title":"Next week"},{"location":"minutes/31-aug-18/","text":"31 August 2018 \u00b6 Create a Conda Virtual Environment \u00b6 Thomas discussed best practices for managing Python packages and recommends nestling all used packages in local Python virtual environments. Thomas highly recommends not using system wide installations. Example conda virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt # install whatever you want via pip Get Started with Kerras and TensorFlow \u00b6 Thomas recommends using Kerras with a TensorFlow backend. Keras is easy to use ImageDateGenerator from Keras lets you augment images. Use fit_generator when using ImageDataGenerator U-Net \u00b6 Achieved an accuracy of 78% on validation set. Incorporating seismic attributes may increase accuracy Image scaling is important.","title":"31 August 2018"},{"location":"minutes/31-aug-18/#31-august-2018","text":"","title":"31 August 2018"},{"location":"minutes/31-aug-18/#create-a-conda-virtual-environment","text":"Thomas discussed best practices for managing Python packages and recommends nestling all used packages in local Python virtual environments. Thomas highly recommends not using system wide installations. Example conda virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt # install whatever you want via pip","title":"Create a Conda Virtual Environment"},{"location":"minutes/31-aug-18/#get-started-with-kerras-and-tensorflow","text":"Thomas recommends using Kerras with a TensorFlow backend. Keras is easy to use ImageDateGenerator from Keras lets you augment images. Use fit_generator when using ImageDataGenerator","title":"Get Started with Kerras and TensorFlow"},{"location":"minutes/31-aug-18/#u-net","text":"Achieved an accuracy of 78% on validation set. Incorporating seismic attributes may increase accuracy Image scaling is important.","title":"U-Net"},{"location":"resources/list/","text":"List of Resources \u00b6 TensorFlow Tutorials \u00b6 Quick videos covering basic ML algorithms Variational Autoencoders from a probabilistic perspective Convolutional Neural Networks Papers \u00b6 Subsurface Structure Analysis Using Computational Interpretation and Learning: A Visual Signal Processing Perspective. A comparison of classification techniques for seismic facies recognition","title":"List of Resources"},{"location":"resources/list/#list-of-resources","text":"TensorFlow","title":"List of Resources"},{"location":"resources/list/#tutorials","text":"Quick videos covering basic ML algorithms Variational Autoencoders from a probabilistic perspective Convolutional Neural Networks","title":"Tutorials"},{"location":"resources/list/#papers","text":"Subsurface Structure Analysis Using Computational Interpretation and Learning: A Visual Signal Processing Perspective. A comparison of classification techniques for seismic facies recognition","title":"Papers"}]}